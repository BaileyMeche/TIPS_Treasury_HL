{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c3c2a0e",
   "metadata": {},
   "source": [
    "\n",
    "# Data Dictionary and Provenance Appendix\n",
    "\n",
    "This notebook documents inputs, transformations, and outputs for the Treasury inflation basis analysis suite. It produces a markdown appendix (`reports/appendix_data_dictionary.md`) and a provenance YAML file (`exports/analysis_artifacts.yml`) containing SHA256 hashes for reproducibility.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96d6cd0",
   "metadata": {},
   "source": [
    "\n",
    "## Imports and helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8355680",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import hashlib\n",
    "import pathlib\n",
    "from typing import Dict\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "INPUT_PATHS = [\n",
    "    pathlib.Path('data/trace_microstructure_event_panels.csv'),\n",
    "    pathlib.Path('data/tenor_liq.csv'),\n",
    "    pathlib.Path('_output/strategy3/state_estimates.csv'),\n",
    "    pathlib.Path('data/policy/treasury_buybacks_refunding.csv'),\n",
    "    pathlib.Path('data/val/bei_ils_wedge_by_tenor.csv'),\n",
    "    pathlib.Path('_output/strategy3/variance_decomposition.csv'),\n",
    "    pathlib.Path('_output/strategy3/halflife_summary.csv')\n",
    "]\n",
    "\n",
    "OUTPUT_PATHS = [\n",
    "    pathlib.Path('reports/microstructure_concentration_results.html'),\n",
    "    pathlib.Path('reports/microstructure_concentration_results.csv'),\n",
    "    pathlib.Path('reports/policy_intervention_state_space.html'),\n",
    "    pathlib.Path('reports/policy_intervention_coeffs.csv'),\n",
    "    pathlib.Path('reports/val_wedge_linkage.html'),\n",
    "    pathlib.Path('reports/val_wedge_linkage.csv'),\n",
    "    pathlib.Path('reports/forecast_comparison.html'),\n",
    "    pathlib.Path('reports/forecast_rmsfe.csv'),\n",
    "    pathlib.Path('reports/event_irfs_daily.html'),\n",
    "    pathlib.Path('reports/event_irfs_daily.csv'),\n",
    "    pathlib.Path('reports/strategy3_state_space.html'),\n",
    "    pathlib.Path('tables/state_space_variance.csv'),\n",
    "    pathlib.Path('reports/appendix_data_dictionary.md'),\n",
    "    pathlib.Path('exports/analysis_artifacts.yml')\n",
    "]\n",
    "\n",
    "OUTPUT_MD = pathlib.Path('reports/appendix_data_dictionary.md')\n",
    "OUTPUT_YAML = pathlib.Path('exports/analysis_artifacts.yml')\n",
    "OUTPUT_MD.parent.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_YAML.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def sha256(path: pathlib.Path) -> str:\n",
    "    if not path.exists():\n",
    "        return ''\n",
    "    h = hashlib.sha256()\n",
    "    with path.open('rb') as f:\n",
    "        for chunk in iter(lambda: f.read(8192), b''):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "\n",
    "def summarize_csv(path: pathlib.Path) -> Dict:\n",
    "    if not path.exists():\n",
    "        return {'path': str(path), 'exists': False, 'columns': [], 'rows': 0}\n",
    "    df = pd.read_csv(path)\n",
    "    return {\n",
    "        'path': str(path),\n",
    "        'exists': True,\n",
    "        'columns': df.columns.tolist(),\n",
    "        'rows': len(df)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f1148b",
   "metadata": {},
   "source": [
    "\n",
    "## Input dataset overview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb71d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_summaries = [summarize_csv(path) for path in INPUT_PATHS]\n",
    "inputs_df = pd.DataFrame(input_summaries)\n",
    "inputs_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee95681",
   "metadata": {},
   "source": [
    "\n",
    "## Output file status\n",
    "\n",
    "This section lists expected analysis artifacts. Files not yet generated will have empty hashes and `exists = False` until the notebooks are executed end-to-end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae6b649",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_records = []\n",
    "for path in OUTPUT_PATHS:\n",
    "    record = {\n",
    "        'path': str(path),\n",
    "        'exists': path.exists(),\n",
    "        'sha256': sha256(path) if path.exists() else ''\n",
    "    }\n",
    "    output_records.append(record)\n",
    "outputs_df = pd.DataFrame(output_records)\n",
    "outputs_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481f6018",
   "metadata": {},
   "source": [
    "\n",
    "## Write markdown appendix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634fd2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lines = ['# Appendix: Data Dictionary', '', '## Inputs']\n",
    "for record in input_summaries:\n",
    "    status = 'available' if record['exists'] else 'missing'\n",
    "    cols = ', '.join(record['columns']) if record['columns'] else 'n/a'\n",
    "    lines.append(f\"- **{record['path']}** ({status}, rows={record['rows']}): columns = {cols}\")\n",
    "lines.append('')\n",
    "lines.append('## Outputs')\n",
    "for record in output_records:\n",
    "    status = 'available' if record['exists'] else 'pending'\n",
    "    lines.append(f\"- **{record['path']}** ({status})\")\n",
    "OUTPUT_MD.write_text('\\n'.join(lines))\n",
    "OUTPUT_MD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e97255",
   "metadata": {},
   "source": [
    "\n",
    "## Write provenance YAML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c431ab41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def format_yaml_list(items, indent=0):\n",
    "    spaces = '  ' * indent\n",
    "    lines = []\n",
    "    for item in items:\n",
    "        lines.append(f\"{spaces}- path: {item['path']}\")\n",
    "        lines.append(f\"{spaces}  exists: {str(item['exists']).lower()}\")\n",
    "        hash_val = item.get('sha256') or sha256(pathlib.Path(item['path']))\n",
    "        if hash_val:\n",
    "            lines.append(f\"{spaces}  sha256: {hash_val}\")\n",
    "        else:\n",
    "            lines.append(f\"{spaces}  sha256: null\")\n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "input_yaml = format_yaml_list(input_summaries)\n",
    "output_yaml = format_yaml_list(output_records)\n",
    "yaml_text = f\"inputs:\\n{input_yaml}\\noutputs:\\n{output_yaml}\\n\"\n",
    "OUTPUT_YAML.write_text(yaml_text)\n",
    "OUTPUT_YAML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4d479b",
   "metadata": {},
   "source": [
    "\n",
    "## Interpretation\n",
    "\n",
    "The appendix confirms all datasets reside within the repository and provides a checklist for verifying generated artifacts. Missing hashes indicate steps that require notebook execution before dissemination.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
