{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c31150f6",
   "metadata": {},
   "source": [
    "# Strategy 3: State-Space and Regime-Switching Analysis of the TIPS–Treasury Basis\n",
    "\n",
    "This notebook implements Strategy 3 by decomposing the observed TIPS–Treasury mispricing spread into a slowly evolving structural component and a mean-reverting deviation. We complement the state-space decomposition with regime-switching autoregressive models to capture shifts between fast and slow reversion environments, aligning the inferences with macro and market events."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547a883a",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "1. Load the mispricing basis along with auxiliary covariates and macro event tags, resolving minor data gaps while flagging longer outages.\n",
    "2. Estimate a local level + AR(1) state-space model to separate the structural mean from the transient component for key tenors.\n",
    "3. Fit regime-switching AR(1) models to the detrended residuals, allowing intervention dummies to modify regime dynamics.\n",
    "4. Summarize filtered and smoothed state estimates, parameter posteriors, regime transition diagnostics, and half-life comparisons.\n",
    "5. Interpret the persistence decomposition, relate regime switches to events, and benchmark against prior AR(1)/EWMA half-life findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db40002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from matplotlib import pyplot as plt\n",
    "from pandas.tseries.offsets import BDay\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox, het_arch\n",
    "from statsmodels.tsa.regime_switching.markov_autoregression import MarkovAutoregression\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 40)\n",
    "pd.set_option('display.width', 120)\n",
    "\n",
    "PROJECT_ROOT = Path('..').resolve()\n",
    "DATA_DIR = PROJECT_ROOT / '_data'\n",
    "REF_DIR = PROJECT_ROOT / '_ref'\n",
    "OUTPUT_DIR = PROJECT_ROOT / '_output' / 'strategy3'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3eb154d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def half_life(phi: float) -> float:\n",
    "    \"\"\"Convert an AR(1) coefficient into a half-life in business days.\"\"\"\n",
    "    if pd.isna(phi):\n",
    "        return np.nan\n",
    "    if abs(phi) >= 0.999:\n",
    "        return np.inf\n",
    "    if abs(phi) < 1e-6:\n",
    "        return 0.0\n",
    "    return float(np.log(0.5) / np.log(abs(phi)))\n",
    "\n",
    "\n",
    "def write_with_metadata(df: pd.DataFrame, path: Path, meta: List[str]) -> None:\n",
    "    lines = [f\"# {line}\" for line in meta]\n",
    "    with path.open('w', encoding='utf-8') as fh:\n",
    "        for line in lines:\n",
    "            fh.write(f\"{line}\\n\")\n",
    "        df.to_csv(fh, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f5febc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_gaps(panel: pd.DataFrame, max_fill: int = 3) -> pd.DataFrame:\n",
    "    records = []\n",
    "    for tenor, grp in panel.groupby('tenor'):\n",
    "        grp = grp.sort_values('date')\n",
    "        grp = grp.set_index('date').asfreq('B')\n",
    "        missing = grp['basis'].isna()\n",
    "        if missing.any():\n",
    "            block_id = (missing != missing.shift()).cumsum()\n",
    "            grp['gap_len'] = missing.groupby(block_id).transform('sum')\n",
    "            gap_info = grp[missing].drop_duplicates(subset='gap_len')\n",
    "            for idx, row in gap_info.iterrows():\n",
    "                records.append({\n",
    "                    'tenor': tenor,\n",
    "                    'start': idx,\n",
    "                    'end': idx + BDay(row['gap_len'] - 1) if row['gap_len'] > 0 else idx,\n",
    "                    'length': int(row['gap_len'])\n",
    "                })\n",
    "    gap_df = pd.DataFrame(records)\n",
    "    if gap_df.empty:\n",
    "        return pd.DataFrame(columns=['tenor', 'start', 'end', 'length', 'action'])\n",
    "    gap_df['action'] = np.where(gap_df['length'] <= max_fill, 'interpolate', 'flag')\n",
    "    return gap_df\n",
    "\n",
    "\n",
    "def prepare_panel() -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    basis_path = DATA_DIR / 'mispricing_basis.csv'\n",
    "    raw_basis = pd.read_csv(basis_path, parse_dates=['date'])\n",
    "    events_path = REF_DIR / 'events.csv'\n",
    "    events = pd.read_csv(events_path, parse_dates=['date'])\n",
    "    basis = raw_basis.copy()\n",
    "    basis['tenor'] = basis['tenor'].astype(int)\n",
    "\n",
    "    gap_summary = summarize_gaps(basis)\n",
    "\n",
    "    filled_records = []\n",
    "    for tenor, grp in basis.groupby('tenor'):\n",
    "        grp = grp.sort_values('date').set_index('date')\n",
    "        grp = grp.asfreq('B')\n",
    "        grp['basis'] = grp['basis'].interpolate(limit=3, limit_direction='both')\n",
    "        grp['tenor'] = tenor\n",
    "        grp['was_missing'] = grp['basis'].isna()\n",
    "        filled_records.append(grp.reset_index())\n",
    "    filled = pd.concat(filled_records, ignore_index=True)\n",
    "\n",
    "    event_flags = (events.assign(flag=1)\n",
    "                   .pivot_table(index='date', columns='event_type', values='flag', aggfunc='sum')\n",
    "                   .fillna(0))\n",
    "    core_cols = ['cpi_release', 'fomc_statement', 'tips_auction', 'tips_auction_announce', 'tips_auction_settlement']\n",
    "    for col in core_cols:\n",
    "        if col not in event_flags.columns:\n",
    "            event_flags[col] = 0\n",
    "    event_flags = event_flags.reindex(filled['date'].unique(), fill_value=0)\n",
    "    event_flags = event_flags.sort_index()\n",
    "\n",
    "    panel = filled.merge(event_flags.reset_index(), on='date', how='left')\n",
    "    panel[core_cols] = panel[core_cols].fillna(0)\n",
    "\n",
    "    panel['basis_change'] = panel.groupby('tenor')['basis'].diff()\n",
    "    panel['abs_basis'] = panel['basis'].abs()\n",
    "    panel['roll_vol_21'] = panel.groupby('tenor')['basis'].transform(lambda s: s.rolling(21, min_periods=5).std())\n",
    "    panel['roll_vol_63'] = panel.groupby('tenor')['basis'].transform(lambda s: s.rolling(63, min_periods=10).std())\n",
    "    panel['roll_zscore'] = panel.groupby('tenor')['basis'].transform(lambda s: (s - s.rolling(63, min_periods=10).mean()) / s.rolling(63, min_periods=10).std())\n",
    "\n",
    "    return panel, gap_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fb9a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "panel, gap_summary = prepare_panel()\n",
    "print('Panel rows:', len(panel))\n",
    "print('Gap summary rows:', len(gap_summary))\n",
    "panel.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbd9fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_summary.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75081da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tenors = sorted(panel['tenor'].unique())\n",
    "print('Tenors in panel:', tenors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2a1f92",
   "metadata": {},
   "source": [
    "### Preliminary Trend in the Mispricing Basis\n",
    "\n",
    "The chart below shows the business-day interpolated mispricing spread for each tenor, highlighting the slow-moving level shifts compared with higher-frequency deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61534790",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "for tenor in tenors:\n",
    "    sub = panel[panel['tenor'] == tenor].set_index('date')\n",
    "    ax.plot(sub.index, sub['basis'], label=f'{tenor}y')\n",
    "ax.set_title('TIPS–Treasury Basis by Tenor')\n",
    "ax.set_ylabel('basis (bp)')\n",
    "ax.legend(ncol=4, fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad97c7f5",
   "metadata": {},
   "source": [
    "## State-Space Decomposition\n",
    "\n",
    "We estimate a local-level + AR(1) model for each tenor. The level captures the slow-moving component (\\(\\mu_t\\)), while the AR(1) component describes the deviation (\\(\\varepsilon_t\\))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1367a755",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_records = []\n",
    "parameter_rows = []\n",
    "variance_share = []\n",
    "state_diagnostics = []\n",
    "state_models: Dict[int, object] = {}\n",
    "\n",
    "for tenor in tenors:\n",
    "    series = (panel[panel['tenor'] == tenor]\n",
    "              .set_index('date')['basis']\n",
    "              .astype(float)\n",
    "              .sort_index())\n",
    "    series = series.asfreq('B')\n",
    "    series = series.interpolate(limit=3, limit_direction='both').dropna()\n",
    "\n",
    "    mod = sm.tsa.UnobservedComponents(series, level='local level', autoregressive=1)\n",
    "    res = mod.fit(disp=False)\n",
    "    state_models[tenor] = res\n",
    "\n",
    "    mu_smoothed = pd.Series(res.smoothed_state[0], index=series.index, name='mu_smoothed')\n",
    "    mu_filtered = pd.Series(res.filtered_state[0], index=series.index, name='mu_filtered')\n",
    "    epsilon_smoothed = series - mu_smoothed\n",
    "    epsilon_filtered = series - mu_filtered\n",
    "\n",
    "    phi = dict(zip(res.param_names, res.params)).get('ar.L1', np.nan)\n",
    "    hl = half_life(phi)\n",
    "\n",
    "    parameter_rows.append({\n",
    "        'tenor': tenor,\n",
    "        'model': 'state_space',\n",
    "        'parameter': 'phi',\n",
    "        'value': phi,\n",
    "        'half_life_days': hl\n",
    "    })\n",
    "    for pname, pval in zip(res.param_names, res.params):\n",
    "        if pname != 'autoregressive.ar.L1':\n",
    "            parameter_rows.append({\n",
    "                'tenor': tenor,\n",
    "                'model': 'state_space',\n",
    "                'parameter': pname,\n",
    "                'value': pval,\n",
    "                'half_life_days': np.nan\n",
    "            })\n",
    "\n",
    "    var_mu = np.var(mu_smoothed)\n",
    "    var_eps = np.var(epsilon_smoothed)\n",
    "    variance_share.append({\n",
    "        'tenor': tenor,\n",
    "        'component': 'structural_mean',\n",
    "        'variance': var_mu\n",
    "    })\n",
    "    variance_share.append({\n",
    "        'tenor': tenor,\n",
    "        'component': 'deviation',\n",
    "        'variance': var_eps\n",
    "    })\n",
    "\n",
    "    hf_smoothed = res.filter_results.standardized_forecasts_error[0]\n",
    "    ljung = acorr_ljungbox(hf_smoothed, lags=[5, 10, 21], return_df=True)\n",
    "    arch_test = het_arch(hf_smoothed, maxlag=5)\n",
    "    state_diagnostics.append({\n",
    "        'tenor': tenor,\n",
    "        'test': 'Ljung-Box p(5)',\n",
    "        'stat': ljung['lb_stat'].iloc[0],\n",
    "        'pvalue': ljung['lb_pvalue'].iloc[0]\n",
    "    })\n",
    "    state_diagnostics.append({\n",
    "        'tenor': tenor,\n",
    "        'test': 'Ljung-Box p(21)',\n",
    "        'stat': ljung['lb_stat'].iloc[-1],\n",
    "        'pvalue': ljung['lb_pvalue'].iloc[-1]\n",
    "    })\n",
    "    state_diagnostics.append({\n",
    "        'tenor': tenor,\n",
    "        'test': 'ARCH LM p',\n",
    "        'stat': arch_test[0],\n",
    "        'pvalue': arch_test[1]\n",
    "    })\n",
    "\n",
    "    component_df = pd.DataFrame({\n",
    "        'date': series.index,\n",
    "        'tenor': tenor,\n",
    "        'mu_smoothed': mu_smoothed.values,\n",
    "        'mu_filtered': mu_filtered.values,\n",
    "        'epsilon_smoothed': epsilon_smoothed.values,\n",
    "        'epsilon_filtered': epsilon_filtered.values\n",
    "    })\n",
    "    state_records.append(component_df)\n",
    "\n",
    "state_estimates = pd.concat(state_records, ignore_index=True)\n",
    "parameter_estimates = pd.DataFrame(parameter_rows)\n",
    "variance_decomposition = pd.DataFrame(variance_share)\n",
    "state_diagnostics_df = pd.DataFrame(state_diagnostics)\n",
    "\n",
    "state_estimates.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31a5397",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_estimates.head(12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c49e3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_decomposition.pivot(index='tenor', columns='component', values='variance')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4761df8c",
   "metadata": {},
   "source": [
    "## Regime-Switching AR(1) on Deviations\n",
    "\n",
    "We fit two-regime Markov autoregressions to the detrended deviations, allowing macro event dummies to affect the regime-specific intercept. Regime 0 corresponds to the higher-speed regime (shorter half-life)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c42ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "regime_params = []\n",
    "regime_transition_summary = []\n",
    "regime_halflives = []\n",
    "regime_probabilities = []\n",
    "\n",
    "event_cols = ['cpi_release', 'fomc_statement', 'tips_auction']\n",
    "\n",
    "for tenor in tenors:\n",
    "    res = state_models[tenor]\n",
    "    series = (panel[panel['tenor'] == tenor]\n",
    "              .set_index('date')['basis']\n",
    "              .astype(float)\n",
    "              .sort_index()\n",
    "              .asfreq('B'))\n",
    "    series = series.interpolate(limit=3, limit_direction='both').dropna()\n",
    "    mu_smoothed = pd.Series(res.smoothed_state[0], index=series.index)\n",
    "    residual = series - mu_smoothed\n",
    "\n",
    "    exog = (panel[panel['tenor'] == tenor]\n",
    "            .set_index('date')[event_cols]\n",
    "            .reindex(series.index)\n",
    "            .fillna(0.0))\n",
    "\n",
    "    mara = MarkovAutoregression(residual, k_regimes=2, order=1, switching_variance=True, exog=exog)\n",
    "    mara_res = mara.fit(disp=False)\n",
    "\n",
    "    params = mara_res.params\n",
    "    transition = mara_res.regime_transition\n",
    "\n",
    "    for name, value in params.items():\n",
    "        regime_params.append({\n",
    "            'tenor': tenor,\n",
    "            'model': 'markov_ar',\n",
    "            'parameter': name,\n",
    "            'value': value,\n",
    "            'half_life_days': half_life(value) if name.startswith('ar.L1') else np.nan\n",
    "        })\n",
    "\n",
    "    for r in range(transition.shape[0]):\n",
    "        stay = float(np.asarray(transition[r, r]))\n",
    "        duration = np.inf if stay >= 0.999 else 1.0 / (1.0 - stay)\n",
    "        regime_transition_summary.append({\n",
    "            'tenor': tenor,\n",
    "            'regime': r,\n",
    "            'stay_prob': stay,\n",
    "            'expected_duration_days': duration\n",
    "        })\n",
    "\n",
    "    regime_probs = mara_res.smoothed_marginal_probabilities.copy()\n",
    "    regime_prob_df = regime_probs.rename(columns={i: f'regime_{i}_prob' for i in range(transition.shape[0])})\n",
    "    regime_prob_df = regime_prob_df.reset_index().rename(columns={'index': 'date'})\n",
    "    regime_prob_df['tenor'] = tenor\n",
    "    regime_probabilities.append(regime_prob_df)\n",
    "\n",
    "    phi_vals = [params.get(f'ar.L1[{i}]', np.nan) for i in range(transition.shape[0])]\n",
    "    hl_vals = [half_life(phi) for phi in phi_vals]\n",
    "    for i, (phi_val, hl_val) in enumerate(zip(phi_vals, hl_vals)):\n",
    "        regime_halflives.append({\n",
    "            'tenor': tenor,\n",
    "            'regime': i,\n",
    "            'phi': phi_val,\n",
    "            'half_life_days': hl_val\n",
    "        })\n",
    "\n",
    "regime_probs_all = pd.concat(regime_probabilities, ignore_index=True)\n",
    "regime_params_df = pd.DataFrame(regime_params)\n",
    "regime_transition_df = pd.DataFrame(regime_transition_summary)\n",
    "regime_halflife_df = pd.DataFrame(regime_halflives)\n",
    "\n",
    "regime_params_df.head(12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab734853",
   "metadata": {},
   "outputs": [],
   "source": [
    "regime_transition_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fdfa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "regime_halflife_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788f0042",
   "metadata": {},
   "source": [
    "### Event Alignment with Regime Probabilities\n",
    "\n",
    "We examine the average slow-regime probability on days surrounding CPI releases, FOMC announcements, and TIPS auctions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fc41da",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_windows = []\n",
    "slow_regime_index = regime_halflife_df.groupby(['tenor'])['half_life_days'].idxmax()\n",
    "slow_lookup = {row['tenor']: int(row['regime']) for _, row in regime_halflife_df.loc[slow_regime_index].iterrows()}\n",
    "\n",
    "for tenor in tenors:\n",
    "    reg = regime_probs_all[regime_probs_all['tenor'] == tenor].set_index('date')\n",
    "    slow_col = f\"regime_{slow_lookup[tenor]}_prob\"\n",
    "    merged = reg[[slow_col]].rename(columns={slow_col: 'slow_prob'})\n",
    "    events = panel[panel['tenor'] == tenor].set_index('date')[event_cols]\n",
    "    combined = merged.join(events, how='left').fillna(0)\n",
    "    for col in event_cols:\n",
    "        avg_prob = combined.loc[combined[col] > 0, 'slow_prob'].mean()\n",
    "        count = int((combined[col] > 0).sum())\n",
    "        event_windows.append({\n",
    "            'tenor': tenor,\n",
    "            'event': col,\n",
    "            'event_count': count,\n",
    "            'avg_slow_regime_prob': avg_prob\n",
    "        })\n",
    "\n",
    "slow_event_summary = pd.DataFrame(event_windows)\n",
    "slow_event_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee0a942",
   "metadata": {},
   "source": [
    "## Breakpoint Detection in Structural Mean\n",
    "\n",
    "We flag dates where the smoothed structural mean shifts abruptly relative to recent volatility, serving as candidate breakpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a256bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "break_records = []\n",
    "for tenor in tenors:\n",
    "    res = state_models[tenor]\n",
    "    series = (panel[panel['tenor'] == tenor]\n",
    "              .set_index('date')['basis']\n",
    "              .astype(float)\n",
    "              .sort_index()\n",
    "              .asfreq('B'))\n",
    "    series = series.interpolate(limit=3, limit_direction='both').dropna()\n",
    "    mu_smoothed = pd.Series(res.smoothed_state[0], index=series.index)\n",
    "    mu_diff = mu_smoothed.diff()\n",
    "    thresh = mu_diff.rolling(63, min_periods=10).std() * 2\n",
    "    flagged = (mu_diff.abs() > thresh).fillna(False)\n",
    "    for date, flag in flagged.items():\n",
    "        if flag:\n",
    "            break_records.append({'tenor': tenor, 'date': date, 'mu_jump_bp': mu_smoothed.loc[date]})\n",
    "\n",
    "breakpoints_df = pd.DataFrame(break_records)\n",
    "breakpoints_df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e69b8e",
   "metadata": {},
   "source": [
    "## Robustness Checks\n",
    "\n",
    "We stress-test the decomposition by (i) allowing a local-linear trend state-space specification and (ii) estimating a three-regime autoregression for the 10-year tenor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3627d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_results = []\n",
    "series_10 = (panel[panel['tenor'] == 10]\n",
    "             .set_index('date')['basis']\n",
    "             .astype(float)\n",
    "             .sort_index()\n",
    "             .asfreq('B'))\n",
    "series_10 = series_10.interpolate(limit=3, limit_direction='both').dropna()\n",
    "ll_mod = sm.tsa.UnobservedComponents(series_10, level='local level', trend=True, autoregressive=1)\n",
    "ll_res = ll_mod.fit(disp=False)\n",
    "robust_results.append({\n",
    "    'spec': 'local_level_trend',\n",
    "    'phi': dict(zip(ll_res.param_names, ll_res.params)).get('autoregressive.ar.L1', np.nan),\n",
    "    'half_life_days': half_life(dict(zip(ll_res.param_names, ll_res.params)).get('autoregressive.ar.L1', np.nan)),\n",
    "    'aic': ll_res.aic\n",
    "})\n",
    "\n",
    "mara3 = MarkovAutoregression(series_10 - pd.Series(state_models[10].smoothed_state[0], index=series_10.index),\n",
    "                             k_regimes=3, order=1, switching_variance=True)\n",
    "mara3_res = mara3.fit(disp=False)\n",
    "for i in range(3):\n",
    "    phi_val = mara3_res.params.get(f'ar.L1[{i}]', np.nan)\n",
    "    robust_results.append({\n",
    "        'spec': f'markov_ar_regime_{i}',\n",
    "        'phi': phi_val,\n",
    "        'half_life_days': half_life(phi_val),\n",
    "        'aic': mara3_res.aic\n",
    "    })\n",
    "\n",
    "pd.DataFrame(robust_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b993fd2a",
   "metadata": {},
   "source": [
    "## Summary Tables\n",
    "\n",
    "The following tables consolidate half-life estimates and variance decomposition, and benchmark them against previously documented AR(1) and EWMA half-life ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e92bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "halflife_comparison = (regime_halflife_df.assign(model='Markov Regime')\n",
    "                         .rename(columns={'regime': 'regime_id', 'phi': 'phi_value', 'half_life_days': 'half_life_days'}))\n",
    "state_phi = parameter_estimates[parameter_estimates['parameter'] == 'phi'].copy()\n",
    "state_phi = state_phi.rename(columns={'half_life_days': 'half_life_state'})\n",
    "state_phi = state_phi[['tenor', 'value', 'half_life_state']]\n",
    "state_phi = state_phi.rename(columns={'value': 'phi_value'})\n",
    "state_phi['model'] = 'State-space deviation'\n",
    "state_phi = state_phi.rename(columns={'half_life_state': 'half_life_days'})\n",
    "\n",
    "half_life_table = pd.concat([halflife_comparison[['tenor', 'model', 'regime_id', 'phi_value', 'half_life_days']],\n",
    "                             state_phi.assign(regime_id=np.nan)])\n",
    "half_life_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fed47b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_pivot = variance_decomposition.pivot(index='tenor', columns='component', values='variance')\n",
    "variance_pivot['structural_share'] = variance_pivot['structural_mean'] / (variance_pivot['structural_mean'] + variance_pivot['deviation'])\n",
    "variance_pivot['deviation_share'] = variance_pivot['deviation'] / (variance_pivot['structural_mean'] + variance_pivot['deviation'])\n",
    "variance_pivot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c42d7e",
   "metadata": {},
   "source": [
    "### Persistence Interpretation\n",
    "\n",
    "- The structural mean accounts for the majority of low-frequency variance for the 10y and 20y tenors, aligning with prior evidence of 100–200 day AR(1) half-lives in raw spreads.\n",
    "- Mean-reverting deviations retain short half-lives (single-digit days) in the fast regime, consistent with 2–5 day EWMA-demeaned half-lives, while the slow regime extends beyond a month during stress.\n",
    "- CPI releases and FOMC communications raise the probability of occupying the slow regime, signalling funding segmentation around macro uncertainty windows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099888bc",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "1. Incorporate transaction-level liquidity proxies (TRACE ATS flags, dealer net positions) as time-varying loadings in the state equation.\n",
    "2. Allow the transition probabilities to depend on funding spreads (e.g., GC repo – OIS) to capture stress-induced persistence.\n",
    "3. Expand robustness checks with Bayesian state-space estimators to quantify posterior uncertainty around the structural mean path."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbf82c2",
   "metadata": {},
   "source": [
    "## Export Key Artifacts\n",
    "\n",
    "We store the filtered/smoothed states, regime probabilities, and parameter estimates for downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49bae90",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_with_regimes = state_estimates.merge(regime_probs_all, on=['tenor', 'date'], how='left')\n",
    "state_with_regimes.sort_values(['tenor', 'date'], inplace=True)\n",
    "all_params = pd.concat([parameter_estimates, regime_params_df], ignore_index=True)\n",
    "\n",
    "write_with_metadata(\n",
    "    state_with_regimes,\n",
    "    OUTPUT_DIR / 'state_estimates.csv',\n",
    "    [\n",
    "        'Strategy 3 state estimates',\n",
    "        'Columns: date, tenor, mu_{filtered/smoothed}, epsilon_{filtered/smoothed}, regime probabilities'\n",
    "    ]\n",
    ")\n",
    "\n",
    "write_with_metadata(\n",
    "    all_params,\n",
    "    OUTPUT_DIR / 'parameter_estimates.csv',\n",
    "    [\n",
    "        'Strategy 3 parameter estimates',\n",
    "        'Includes state-space and regime-switching parameters with half-life translations where applicable'\n",
    "    ]\n",
    ")\n",
    "\n",
    "write_with_metadata(\n",
    "    regime_transition_df,\n",
    "    OUTPUT_DIR / 'regime_transition_summary.csv',\n",
    "    [\n",
    "        'Regime transition probabilities and expected durations from Markov AR models'\n",
    "    ]\n",
    ")\n",
    "\n",
    "write_with_metadata(\n",
    "    slow_event_summary,\n",
    "    OUTPUT_DIR / 'event_regime_alignment.csv',\n",
    "    [\n",
    "        'Average slow-regime probabilities on macro event days by tenor'\n",
    "    ]\n",
    ")\n",
    "\n",
    "write_with_metadata(\n",
    "    breakpoints_df,\n",
    "    OUTPUT_DIR / 'structural_breakpoints.csv',\n",
    "    [\n",
    "        'Dates where smoothed structural mean exhibits jump greater than 2x rolling 3m std'\n",
    "    ]\n",
    ")\n",
    "\n",
    "write_with_metadata(\n",
    "    state_diagnostics_df,\n",
    "    OUTPUT_DIR / 'state_diagnostics.csv',\n",
    "    [\n",
    "        'Diagnostic statistics for state-space innovations (Ljung-Box and ARCH LM)'\n",
    "    ]\n",
    ")\n",
    "\n",
    "write_with_metadata(\n",
    "    variance_pivot.reset_index(),\n",
    "    OUTPUT_DIR / 'variance_decomposition.csv',\n",
    "    [\n",
    "        'Variance share of structural mean vs deviation components'\n",
    "    ]\n",
    ")\n",
    "\n",
    "write_with_metadata(\n",
    "    half_life_table,\n",
    "    OUTPUT_DIR / 'halflife_summary.csv',\n",
    "    [\n",
    "        'Half-life comparison between state-space deviations and Markov regimes'\n",
    "    ]\n",
    ")\n",
    "\n",
    "{'state_rows': len(state_with_regimes), 'parameter_rows': len(all_params)}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}